###############################################################################
#
# Flume / Kafka ingest pipeline, offering 3 broadcast channels that can be 
# tapped with differing qualities of service:
#
#   1. single/non-durable, optimised for lowest latency delivery of single events,
#      add memory channel to cyclehire.sources.source_single.channels
#   2. single/durable, optimised for low latency delivery of single events with
#      durable operation, add kafka source to topic cyclehire_single
#   3. batch/durable, optimised for greatest throughput of batch events with
#      durable operation, add kafka source to topic cyclehire_batch
#
###############################################################################

cyclehire.sources=source_single source_batch
cyclehire.channels=channel_single_nondurable_hdfs channel_single_durable channel_batch_durable
cyclehire.sinks=sink_single_hdfs sink_batch_hdfs

cyclehire.sources.source_single.type=com.cloudera.cyclehire.main.ingress.stream.StreamSource
cyclehire.sources.source_single.httpUrl=http://www.tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml
cyclehire.sources.source_single.pollMs=1000
cyclehire.sources.source_single.pollTicks=0
cyclehire.sources.source_single.batchSize=1
cyclehire.sources.source_single.interceptors=interceptor_stream
cyclehire.sources.source_single.interceptors.interceptor_stream.type=com.cloudera.cyclehire.main.ingress.stream.StreamInterceptor$Builder
cyclehire.sources.source_single.selector.type=replicating
cyclehire.sources.source_single.channels=channel_single_nondurable_hdfs channel_single_durable

cyclehire.channels.channel_single_nondurable_hdfs.type=memory
cyclehire.channels.channel_single_nondurable_hdfs.capacity=10000
cyclehire.channels.channel_single_nondurable_hdfs.transactionCapacity=1000
cyclehire.channels.channel_single_nondurable_hdfs.keep-alive=1
cyclehire.channels.channel_single_nondurable_hdfs.byteCapacityBufferPercentage=1

cyclehire.sinks.sink_single_hdfs.type=hdfs
cyclehire.sinks.sink_single_hdfs.hdfs.path=hdfs://$HDFS_NAMENODE_HOST$ROOT_DIR_HDFS_RAW_LANDED_XML/none/%{ch_batch}_livecyclehireupdates-%{ch_host}.xml
cyclehire.sinks.sink_single_hdfs.hdfs.filePrefix=%{ch_timestamp}_livecyclehireupdates-%{ch_index}-of-%{ch_total}
cyclehire.sinks.sink_single_hdfs.hdfs.fileSuffix=.xml
cyclehire.sinks.sink_single_hdfs.hdfs.inUsePrefix=_
cyclehire.sinks.sink_single_hdfs.hdfs.rollCount=0
cyclehire.sinks.sink_single_hdfs.hdfs.rollInterval=0
cyclehire.sinks.sink_single_hdfs.hdfs.rollSize=0
cyclehire.sinks.sink_single_hdfs.hdfs.idleTimeout=1
cyclehire.sinks.sink_single_hdfs.hdfs.batchSize=1
cyclehire.sinks.sink_single_hdfs.hdfs.writeFormat=Text
cyclehire.sinks.sink_single_hdfs.hdfs.fileType=DataStream
cyclehire.sinks.sink_single_hdfs.channel=channel_single_nondurable_hdfs

cyclehire.channels.channel_single_durable.type=org.apache.flume.channel.kafka.KafkaChannel
cyclehire.channels.channel_single_durable.brokerList=$KAFKA_KAFKA_BROKER_HOSTS_AND_PORTS
cyclehire.channels.channel_single_durable.zookeeperConnect=$ZOOKEEPER_SERVER_HOSTS_AND_PORTS
cyclehire.channels.channel_single_durable.parseAsFlumeEvent=true
cyclehire.channels.channel_single_durable.readSmallestOffset=false
cyclehire.channels.channel_single_durable.capacity=10000
cyclehire.channels.channel_single_durable.transactionCapacity=1000
cyclehire.channels.channel_single_durable.topic=cyclehire_single

cyclehire.sources.source_batch.type=org.apache.flume.source.kafka.KafkaSource
cyclehire.sources.source_batch.zookeeperConnect=$ZOOKEEPER_SERVER_HOSTS_AND_PORTS
cyclehire.sources.source_batch.groupId=source_batch
cyclehire.sources.source_batch.topic=cyclehire_single
cyclehire.sources.source_batch.batchSize=10
cyclehire.sources.source_batch.batchDurationMillis=700000
cyclehire.sources.source_batch.interceptors=interceptor_unwrap interceptor_stream
cyclehire.sources.source_batch.interceptors.interceptor_unwrap.type=com.cloudera.framework.main.common.flume.FlumeEventUnwrapInterceptor$Builder
cyclehire.sources.source_batch.interceptors.interceptor_stream.type=com.cloudera.cyclehire.main.ingress.stream.StreamInterceptor$Builder
cyclehire.sources.source_batch.channels=channel_batch_durable

cyclehire.channels.channel_batch_durable.type=org.apache.flume.channel.kafka.KafkaChannel
cyclehire.channels.channel_batch_durable.brokerList=$KAFKA_KAFKA_BROKER_HOSTS_AND_PORTS
cyclehire.channels.channel_batch_durable.zookeeperConnect=$ZOOKEEPER_SERVER_HOSTS_AND_PORTS
cyclehire.channels.channel_batch_durable.parseAsFlumeEvent=true
cyclehire.channels.channel_batch_durable.readSmallestOffset=false
cyclehire.channels.channel_batch_durable.capacity=10000
cyclehire.channels.channel_batch_durable.transactionCapacity=1000
cyclehire.channels.channel_batch_durable.topic=cyclehire_batch

cyclehire.sinks.sink_batch_hdfs.type=hdfs
cyclehire.sinks.sink_batch_hdfs.hdfs.path=hdfs://$HDFS_NAMENODE_HOST$ROOT_DIR_HDFS_RAW_LANDED_SEQ/none/%{ch_batch}_livecyclehireupdates-%{ch_host}.seq
cyclehire.sinks.sink_batch_hdfs.hdfs.filePrefix=%{ch_batch}_livecyclehireupdates
cyclehire.sinks.sink_batch_hdfs.hdfs.fileSuffix=.seq
cyclehire.sinks.sink_batch_hdfs.hdfs.inUsePrefix=_
cyclehire.sinks.sink_batch_hdfs.hdfs.rollCount=0
cyclehire.sinks.sink_batch_hdfs.hdfs.rollInterval=0
cyclehire.sinks.sink_batch_hdfs.hdfs.rollSize=0
cyclehire.sinks.sink_batch_hdfs.hdfs.idleTimeout=10
cyclehire.sinks.sink_batch_hdfs.hdfs.batchSize=10
cyclehire.sinks.sink_batch_hdfs.hdfs.writeFormat=Writable
cyclehire.sinks.sink_batch_hdfs.hdfs.fileType=SequenceFile
cyclehire.sinks.sink_batch_hdfs.channel=channel_batch_durable
